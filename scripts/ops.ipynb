{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Set the root directory assuming the script is run from within the 'functions' directory.\n",
    "ROOT = Path().resolve().parent\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Retrieve the database connection URL from the environment variables.\n",
    "db_url = os.getenv('DATABASE_URI')\n",
    "\n",
    "# Define the clean_column_names function.\n",
    "def clean_column_names(col_name):\n",
    "    col_name = col_name.split(':', 1)[-1]\n",
    "    col_name = col_name.split(';', 1)[0]\n",
    "    return col_name.strip()\n",
    "\n",
    "# Define a function to hash column names if they exceed the length limit\n",
    "def hash_column_name(col_name, max_length=63):\n",
    "    if len(col_name) > max_length:\n",
    "        hash_object = hashlib.sha256(col_name.encode())\n",
    "        hashed_col_name = \"hashed_\" + hash_object.hexdigest()[:max_length-7]\n",
    "        return hashed_col_name\n",
    "    return col_name\n",
    "\n",
    "def create_mappings_table(cursor):\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS column_name_mappings (\n",
    "        original_name TEXT PRIMARY KEY,\n",
    "        shortened_name TEXT NOT NULL\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "def load_existing_mappings(cursor):\n",
    "    cursor.execute(\"SELECT original_name, shortened_name FROM column_name_mappings\")\n",
    "    return {row[0]: row[1] for row in cursor.fetchall()}\n",
    "\n",
    "def insert_mapping(cursor, original_name, shortened_name):\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO column_name_mappings (original_name, shortened_name)\n",
    "    VALUES (%s, %s)\n",
    "    ON CONFLICT (original_name) DO NOTHING;\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_query, (original_name, shortened_name))\n",
    "\n",
    "def process_csv_file(csv_file, existing_mappings):\n",
    "    print(f\"Processing file {csv_file}\")\n",
    "    try:\n",
    "        # Connect to the PostgreSQL database\n",
    "        conn = psycopg2.connect(db_url)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Read the file into a pandas DataFrame.\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Drop the 'date' column.\n",
    "        if 'date' in df.columns:\n",
    "            print(f\"Dropping 'date' column\")\n",
    "            df = df.drop(columns=['date'])\n",
    "        \n",
    "        # Clean the column names using the clean_column_names function.\n",
    "        df.columns = [clean_column_names(col) for col in df.columns]\n",
    "        \n",
    "        # Rename specific columns.\n",
    "        if 'geography code' in df.columns:\n",
    "            print(f\"Renaming 'geography code' to 'geocode'\")\n",
    "            df = df.rename(columns={'geography code': 'geocode'})\n",
    "        if 'geography' in df.columns:\n",
    "            print(f\"Renaming 'geography' to 'geoname'\")\n",
    "            df = df.rename(columns={'geography': 'geoname'})\n",
    "        \n",
    "        # Shorten column names if necessary and update the mapping\n",
    "        shortened_columns = []\n",
    "        for col in df.columns:\n",
    "            if col in existing_mappings:\n",
    "                shortened_col = existing_mappings[col]\n",
    "            else:\n",
    "                shortened_col = hash_column_name(col)\n",
    "                if col != shortened_col:\n",
    "                    insert_mapping(cursor, col, shortened_col)\n",
    "                    existing_mappings[col] = shortened_col\n",
    "            shortened_columns.append(shortened_col)\n",
    "        \n",
    "        df.columns = shortened_columns\n",
    "        \n",
    "        # Reorder the columns.\n",
    "        cols = df.columns.tolist()\n",
    "        if 'geocode' in cols and 'geoname' in cols and 'Total' in cols:\n",
    "            cols = ['geocode', 'geoname', 'Total'] + [col for col in cols if col not in ['geocode', 'geoname', 'Total']]\n",
    "            df = df[cols]\n",
    "        \n",
    "        # Print the table name and columns to be inserted.\n",
    "        table_name = csv_file.stem\n",
    "        print(f\"Table name: {table_name}\")\n",
    "        print(f\"Columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        # Drop the existing table if it exists.\n",
    "        cursor.execute(sql.SQL(\"DROP TABLE IF EXISTS {}\").format(sql.Identifier(table_name)))\n",
    "        \n",
    "        # Create a new table with the appropriate schema based on the DataFrame.\n",
    "        create_table_query = sql.SQL(\n",
    "            \"CREATE TABLE {} ({}, UNIQUE (geocode), UNIQUE (geoname))\"\n",
    "        ).format(\n",
    "            sql.Identifier(table_name),\n",
    "            sql.SQL(', ').join(\n",
    "                sql.SQL(\"{} {}\").format(\n",
    "                    sql.Identifier(col),\n",
    "                    sql.SQL(\"TEXT\") if col in ['geocode', 'geoname'] else sql.SQL(\"INTEGER\")\n",
    "                ) for col in df.columns\n",
    "            )\n",
    "        )\n",
    "        cursor.execute(create_table_query)\n",
    "        \n",
    "        # Convert all columns except 'geocode' and 'geoname' to integers\n",
    "        for col in df.columns:\n",
    "            if col not in ['geocode', 'geoname']:\n",
    "                df[col] = df[col].astype(int)\n",
    "        \n",
    "        # Use StringIO to create an in-memory CSV\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        csv_buffer.seek(0)\n",
    "        \n",
    "        # Use the COPY command for efficient bulk loading of the CSV data into the table.\n",
    "        cursor.copy_expert(\n",
    "            sql.SQL(\"COPY {} FROM STDIN WITH CSV HEADER\").format(sql.Identifier(table_name)),\n",
    "            csv_buffer\n",
    "        )\n",
    "        \n",
    "        # Commit the transaction.\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} created and data inserted successfully.\")\n",
    "        \n",
    "        # Fetch and print the updated column names from the database\n",
    "        cursor.execute(sql.SQL(\"SELECT column_name FROM information_schema.columns WHERE table_name = {}\").format(sql.Literal(table_name)))\n",
    "        columns = cursor.fetchall()\n",
    "        print(f\"Updated columns in table {table_name}: {', '.join(col[0] for col in columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Set the path to the directory containing subdirectories with CSV files.\n",
    "csv_dir = ROOT / 'data' / 'census' / 'EW'\n",
    "\n",
    "# Collect all CSV files from all subdirectories\n",
    "csv_files = [csv_file for sub_dir in csv_dir.iterdir() if sub_dir.is_dir() for csv_file in sub_dir.glob('*.csv')]\n",
    "\n",
    "# Specify the number of worker threads\n",
    "max_workers = 10\n",
    "\n",
    "# Use ThreadPoolExecutor to process CSV files in parallel\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Connect to the database once to create the mappings table and load existing mappings\n",
    "    conn = psycopg2.connect(db_url)\n",
    "    cursor = conn.cursor()\n",
    "    create_mappings_table(cursor)\n",
    "    existing_mappings = load_existing_mappings(cursor)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    futures = [executor.submit(process_csv_file, csv_file, existing_mappings) for csv_file in csv_files]\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            future.result()  # This will raise any exceptions caught during the execution\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {e}\")\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set the root directory assuming the script is run from within the 'functions' directory.\n",
    "ROOT = Path().resolve().parent\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Retrieve the database connection URL from the environment variables.\n",
    "db_url = os.getenv('DATABASE_URI')\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(ROOT / 'data' / 'census' / 'EW' / 'ew_all_geographies.csv')\n",
    "\n",
    "# Drop the empty columns\n",
    "columns_to_drop = ['ltla22nmw', 'utla22nmw', 'rgn22nmw']\n",
    "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Define a function to determine the partition based on region code\n",
    "def get_partition(rgn22cd):\n",
    "    partitions = {\n",
    "        'E12000001': 'ew_geographies_region1',\n",
    "        'E12000002': 'ew_geographies_region2',\n",
    "        'E12000003': 'ew_geographies_region3',\n",
    "        'E12000004': 'ew_geographies_region4',\n",
    "        'E12000005': 'ew_geographies_region5',\n",
    "        'E12000006': 'ew_geographies_region6',\n",
    "        'E12000007': 'ew_geographies_region7',\n",
    "        'E12000008': 'ew_geographies_region8',\n",
    "        'E12000009': 'ew_geographies_region9',\n",
    "        'W99999999': 'ew_geographies_region10'\n",
    "    }\n",
    "    return partitions.get(rgn22cd, None)\n",
    "\n",
    "# Group the DataFrame by partition\n",
    "df['partition'] = df['rgn22cd'].apply(get_partition)\n",
    "partitions = df.groupby('partition')\n",
    "\n",
    "# Insert data into the appropriate partition in bulk\n",
    "for partition, data in partitions:\n",
    "    if partition:\n",
    "        data.drop(columns=['partition'], inplace=True)\n",
    "        try:\n",
    "            data.to_sql(partition, engine, if_exists='append', index=False, method='multi')\n",
    "        except IntegrityError as e:\n",
    "            print(f\"IntegrityError for partition {partition}: {e}\")\n",
    "            # Optionally, you can handle the error by logging or taking other actions\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set the root directory assuming the script is run from within the 'functions' directory.\n",
    "ROOT = Path().resolve().parent\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Retrieve the database connection URL from the environment variables.\n",
    "db_url = os.getenv('DATABASE_URI')\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(ROOT / 'data' / 'elections' / 'ew_political_boundaries.csv')\n",
    "\n",
    "# Drop the 'ObjectId' column\n",
    "df.drop(columns=['ObjectId'], inplace=True, errors='ignore')\n",
    "\n",
    "# Lowercase all column names\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df.drop_duplicates(subset=['wd22cd'], inplace=True)\n",
    "\n",
    "# Define the metadata\n",
    "metadata = MetaData()\n",
    "\n",
    "# Define the ew_political_boundaries table\n",
    "ew_political_boundaries = Table(\n",
    "    'ew_political_boundaries', metadata,\n",
    "    sqlalchemy.Column('wd22cd', sqlalchemy.String, primary_key=True),\n",
    "    sqlalchemy.Column('wd22nm', sqlalchemy.String),\n",
    "    sqlalchemy.Column('pcon22cd', sqlalchemy.String),\n",
    "    sqlalchemy.Column('pcon22nm', sqlalchemy.String),\n",
    "    sqlalchemy.Column('lad22cd', sqlalchemy.String),\n",
    "    sqlalchemy.Column('lad22nm', sqlalchemy.String),\n",
    "    sqlalchemy.Column('utla22cd', sqlalchemy.String),\n",
    "    sqlalchemy.Column('utla22nm', sqlalchemy.String)\n",
    ")\n",
    "\n",
    "# Create the ew_political_boundaries table in the database\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Insert data into the ew_political_boundaries table\n",
    "try:\n",
    "    df.to_sql('ew_political_boundaries', engine, if_exists='append', index=False, method='multi')\n",
    "except IntegrityError as e:\n",
    "    print(f\"IntegrityError: {e}\")\n",
    "    # Optionally, you can handle the error by logging or taking other actions\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 18:07:43,570 - INFO - Data loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     pconyynm_year                    pconyynm  year pconyycd  \\\n",
      "0             Battersea South_1918             Battersea South  1918     None   \n",
      "1      Bermondsey Rotherhithe_1918      Bermondsey Rotherhithe  1918     None   \n",
      "2  Bermondsey West Bermondsey_1918  Bermondsey West Bermondsey  1918     None   \n",
      "3    Bethnal Green North East_1918    Bethnal Green North East  1918     None   \n",
      "4    Bethnal Green South West_1918    Bethnal Green South West  1918     None   \n",
      "\n",
      "  seats  county     rgn electorate conservative_party_votes  \\\n",
      "0     1  London  London      43036                    15670   \n",
      "1     1  London  London      25008                     5639   \n",
      "2     1  London  London      23100                     None   \n",
      "3     1  London  London      25253                     None   \n",
      "4     1  London  London      19510                     4240   \n",
      "\n",
      "  vote_share_conservative_party_votes  ... vote_share_uu_votes  \\\n",
      "0                  0.6818082930861942  ...                None   \n",
      "1                                 0.5  ...                None   \n",
      "2                                None  ...                None   \n",
      "3                                None  ...                None   \n",
      "4                  0.5224248398225727  ...                None   \n",
      "\n",
      "  liberal_democrats_votes vote_share_liberal_democrats_votes votes_share  \\\n",
      "0                    None                               None        None   \n",
      "1                    None                               None        None   \n",
      "2                    None                               None        None   \n",
      "3                    None                               None        None   \n",
      "4                    None                               None        None   \n",
      "\n",
      "  ukip_votes vote_share_ukip_votes green_votes vote_share_green_votes  \\\n",
      "0       None                  None        None                   None   \n",
      "1       None                  None        None                   None   \n",
      "2       None                  None        None                   None   \n",
      "3       None                  None        None                   None   \n",
      "4       None                  None        None                   None   \n",
      "\n",
      "  brexit_votes vote_share_brexit_votes  \n",
      "0         None                    None  \n",
      "1         None                    None  \n",
      "2         None                    None  \n",
      "3         None                    None  \n",
      "4         None                    None  \n",
      "\n",
      "[5 rows x 119 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Table, MetaData, Column, String, Integer, Float\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "import logging\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Retrieve the database connection URL from the environment variables.\n",
    "db_url = os.getenv('DATABASE_URI')\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Function to convert to proper noun casing\n",
    "def proper_noun_casing(name):\n",
    "    if isinstance(name, str):\n",
    "        words = name.split()\n",
    "        return ' '.join([word.capitalize() if word.lower() != 'and' else word for word in words])\n",
    "    return name\n",
    "\n",
    "# Function to make vote_share columns unique by appending the previous column name\n",
    "def make_vote_share_columns_unique(df):\n",
    "    new_columns = []\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if 'vote share' in col.lower():\n",
    "            if i > 0:\n",
    "                new_col = f\"vote_share_{df.columns[i-1].lower().replace(' ', '_')}\"\n",
    "            else:\n",
    "                new_col = f\"vote_share_{i}\"\n",
    "            new_columns.append(new_col)\n",
    "        else:\n",
    "            new_columns.append(col.lower().replace(' ', '_'))\n",
    "    df.columns = new_columns\n",
    "\n",
    "# Function to remove unnamed columns\n",
    "def remove_unnamed_columns(df):\n",
    "    unnamed_columns = [col for col in df.columns if col.lower().startswith('unnamed') or col.strip() == '']\n",
    "    if unnamed_columns:\n",
    "        df.drop(columns=unnamed_columns, inplace=True)\n",
    "\n",
    "ROOT = Path().resolve().parent\n",
    "excel_file = ROOT / 'data' / 'elections' / 'election-results.xlsx'\n",
    "\n",
    "# Read all sheet names\n",
    "sheet_names = pd.ExcelFile(excel_file).sheet_names\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for sheet in sheet_names:\n",
    "    if sheet == 'University Seats':\n",
    "        continue\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet, header=None)\n",
    "    \n",
    "    # Remove empty columns & rows\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df = df.dropna(axis=0, how='all')\n",
    "    df = df.dropna(thresh=3, axis=1)\n",
    "    df = df.dropna(thresh=3, axis=0)\n",
    "    \n",
    "    # Merge the first and second rows to create column headers\n",
    "    new_headers = df.iloc[0].fillna('') + ' ' + df.iloc[1].fillna('')\n",
    "    df.columns = new_headers.str.strip()  # Remove leading and trailing whitespace\n",
    "    df = df[2:]\n",
    "    \n",
    "    # Drop the 'id' column if it exists\n",
    "    df.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Ensure all column names are strings to avoid AttributeError\n",
    "    df.columns = df.columns.map(str)\n",
    "    \n",
    "    # Rename columns\n",
    "    df = df.rename(columns={\n",
    "        'ONS id': 'pconyycd',\n",
    "        'Constituency': 'pconyynm',\n",
    "        'Country/Region': 'rgn',\n",
    "        'Country': 'ctry'\n",
    "    })\n",
    "    \n",
    "    # Apply proper noun casing to 'pconyynm' column\n",
    "    if 'pconyynm' in df.columns:\n",
    "        df['pconyynm'] = df['pconyynm'].apply(lambda x: proper_noun_casing(str(x)))\n",
    "    \n",
    "    # Make vote_share columns unique\n",
    "    make_vote_share_columns_unique(df)\n",
    "    \n",
    "    # Remove unnamed columns\n",
    "    remove_unnamed_columns(df)\n",
    "    \n",
    "    # Add a 'year' column\n",
    "    df['year'] = sheet\n",
    "    \n",
    "    # Create a unique 'pconyynm_year' column\n",
    "    if 'pconyynm' in df.columns:\n",
    "        df['pconyynm_year'] = df['pconyynm'] + '_' + df['year'].astype(str)\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    all_data.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Ensure all column names are in lowercase\n",
    "combined_df.columns = combined_df.columns.str.lower()\n",
    "\n",
    "# Reorder the columns\n",
    "ordered_columns = ['pconyynm_year', 'pconyynm', 'year', 'pconyycd'] + [col for col in combined_df.columns if col not in ['pconyynm_year', 'pconyynm', 'year', 'pconyycd']]\n",
    "combined_df = combined_df[ordered_columns]\n",
    "\n",
    "# Remove any columns with blank names\n",
    "combined_df = combined_df.loc[:, combined_df.columns.str.strip() != '']\n",
    "\n",
    "# Check for duplicates in 'pconyynm_year' and append 'rgn' to make them unique\n",
    "duplicates = combined_df[combined_df.duplicated('pconyynm_year', keep=False)]\n",
    "if not duplicates.empty:\n",
    "    combined_df.loc[combined_df.duplicated('pconyynm_year', keep=False), 'pconyynm_year'] = (\n",
    "        combined_df['pconyynm_year'] + '_' + combined_df['rgn']\n",
    "    )\n",
    "\n",
    "# Save the combined DataFrame to a single CSV file\n",
    "combined_csv_file = ROOT / 'data' / 'elections' / 'election-results-combined.csv'\n",
    "combined_df.to_csv(combined_csv_file, index=False)\n",
    "\n",
    "# Define the metadata\n",
    "metadata = MetaData()\n",
    "\n",
    "# Define the election_results table\n",
    "election_results = Table(\n",
    "    'election_results', metadata,\n",
    "    Column('pconyynm_year', String, primary_key=True),\n",
    "    Column('pconyynm', String),\n",
    "    Column('year', String),\n",
    "    Column('pconyycd', String),\n",
    "    # Add other columns as needed\n",
    "    *(Column(col, String) for col in combined_df.columns if col not in ['pconyynm_year', 'pconyynm', 'year', 'pconyycd'])\n",
    ")\n",
    "\n",
    "# Create the table in the remote database\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Use COPY to load data into the PostgreSQL database\n",
    "with engine.connect() as connection:\n",
    "    with open(combined_csv_file, 'r') as f:\n",
    "        # Skip the header row for COPY command\n",
    "        next(f)\n",
    "        # COPY command to load the data from the CSV file\n",
    "        copy_sql = f\"\"\"\n",
    "        COPY election_results FROM stdin WITH CSV HEADER\n",
    "        DELIMITER as ','\n",
    "        \"\"\"\n",
    "        try:\n",
    "            connection.connection.cursor().copy_expert(sql=copy_sql, file=f)\n",
    "            connection.connection.commit()\n",
    "            logging.info(\"Data loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data: {e}\")\n",
    "            connection.connection.rollback()\n",
    "\n",
    "# Verify the data has been added\n",
    "query = \"SELECT * FROM election_results LIMIT 10;\"\n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql(query, connection)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted /Users/cardigan/llm-datawarehouse/data/elections/election-results-University Seats.csv\n"
     ]
    }
   ],
   "source": [
    "#  Not implemented - university seats are currently excluded\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def format_university_seats_csv(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # Remove empty rows and columns\n",
    "    df = df.dropna(axis=0, how='all').dropna(axis=1, how='all')\n",
    "    \n",
    "    # Save the formatted DataFrame back to CSV\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def main():\n",
    "    ROOT = Path().resolve().parent\n",
    "    csv_file = ROOT / 'data' / 'elections' / 'election-results-University Seats.csv'\n",
    "    \n",
    "    format_university_seats_csv(csv_file)\n",
    "    print(f\"Formatted {csv_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully into table_titles.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Retrieve the database connection URL from the environment variables.\n",
    "db_url = os.getenv('DATABASE_URI')\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = Path('../api/table_titles.json')\n",
    "\n",
    "# Load JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to create the table_titles table\n",
    "def create_table_titles_table(cursor):\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS table_titles (\n",
    "        country TEXT NOT NULL,\n",
    "        code TEXT NOT NULL,\n",
    "        name TEXT NOT NULL,\n",
    "        PRIMARY KEY (country, code)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "# Function to insert data into the table_titles table\n",
    "def insert_table_titles_data(cursor, country, name, code):\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO table_titles (country, code, name)\n",
    "    VALUES (%s, %s, %s)\n",
    "    ON CONFLICT (country, code) DO NOTHING;\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_query, (country, code.lower(), name))\n",
    "\n",
    "# Main function to process the JSON data and insert it into the table\n",
    "def process_json_data(data):\n",
    "    try:\n",
    "        # Connect to the PostgreSQL database\n",
    "        conn = psycopg2.connect(db_url)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create the table_titles table if it doesn't exist\n",
    "        create_table_titles_table(cursor)\n",
    "        \n",
    "        # Insert data into the table_titles table\n",
    "        for country, titles in data.items():\n",
    "            for name, code in titles.items():\n",
    "                insert_table_titles_data(cursor, country, code, name)\n",
    "        \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(\"Data inserted successfully into table_titles.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSON data: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Process the JSON data\n",
    "process_json_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('admin', '$2b$12$xKWIW1h1Jx46supKSJmmV.ccYW68XrO6i9Tma8Z/nJUP7qbdDKtgC')\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import bcrypt\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Retrieve the database connection URL from the environment variables.\n",
    "db_url = os.getenv('DATABASE_URI')\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(db_url, sslmode='require')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create the 'users' table with a larger password column\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    username VARCHAR(50) PRIMARY KEY,\n",
    "    password VARCHAR(60) NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Hash the password\n",
    "password = 'password'\n",
    "hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n",
    "\n",
    "# Insert entries into the 'users' table\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO users (username, password) VALUES (%s, %s)\n",
    "ON CONFLICT (username) DO NOTHING;\n",
    "\"\"\"\n",
    "cur.execute(insert_query, ('admin', hashed_password.decode('utf-8')))\n",
    "conn.commit()\n",
    "\n",
    "# Verify the insertion\n",
    "cur.execute(\"SELECT * FROM users;\")\n",
    "rows = cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('data/Scotland/OA_TO_HIGHER_AREAS.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    # Read header row to get column names\n",
    "    headers = next(reader)\n",
    "    \n",
    "    # Initialize dictionary to store unique values for each column\n",
    "    unique_values = {col: set() for col in headers}\n",
    "    \n",
    "    # Iterate over each row\n",
    "    for row in reader:\n",
    "        # Iterate over each column in the row\n",
    "        for col, value in zip(headers, row):\n",
    "            unique_values[col].add(value)\n",
    "\n",
    "# Print number of unique values for each column            \n",
    "for col, values in unique_values.items():\n",
    "    print(f\"{col}: {len(values)} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alliance', 'alliance_(northern_ireland)', 'common_wealth_movement', 'communist', 'constitutionalist', 'dup_(uuuc)', 'independent', 'independent_conservative', 'independent_labour', 'independent_labour_party', 'independent_liberal', 'independent_nationalist', 'independent_unionist', 'labour_unionist_(ireland)', 'national', 'national_independent', 'national_labour', 'national_liberal', 'national_liberal/national_liberal_and_conservative', 'nationalist(wales/scotland)', 'nationalist_(ireland)', 'northern_ireland_labour_party', 'other', 'oup', 'oup_(uuuc)', 'pc/snp', 'sdlp', 'sf', 'sinn_fein', 'snp/plaid_cymru', 'ukup', 'unionist(pro-assembly)', 'unionist_(uuuc)', 'uu', 'uup', 'vanguard_unionist_progressive_party_(uuuc)', 'workers_party']\n"
     ]
    }
   ],
   "source": [
    "# Define the list of historic UK political parties\n",
    "party_columns = [\n",
    "    'independent_labour', 'independent_conservative', 'independent_unionist', 'labour_unionist_(ireland)', 'independent_liberal', 'independent', 'national',\n",
    "'sinn_fein', 'nationalist_(ireland)', 'independent_nationalist', 'other',\n",
    "'national_liberal', 'communist', 'constitutionalist', 'national_labour',\n",
    "'independent_labour_party', 'pc/snp', 'national_independent', 'common_wealth_movement',\n",
    "'northern_ireland_labour_party', 'national_liberal/national_liberal_and_conservative',\n",
    "'nationalist(wales/scotland)', 'snp/plaid_cymru',\n",
    "'unionist(pro-assembly)', 'dup_(uuuc)', 'vanguard_unionist_progressive_party_(uuuc)',\n",
    "'unionist_(uuuc)', 'sdlp', 'alliance', 'oup_(uuuc)', 'oup',\n",
    "'alliance_(northern_ireland)', 'workers_party', 'uup',\n",
    "'sf', 'ukup', 'uu'\n",
    "]\n",
    "\n",
    "# Sort the list alphabetically\n",
    "sorted_party_columns = sorted(party_columns)\n",
    "\n",
    "# Print the sorted list\n",
    "print(sorted_party_columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
